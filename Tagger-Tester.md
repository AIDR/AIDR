The tagger tester is a program that can be used after deploying the tagger, to test it independently of the other modules.

It requires both the **tagger stand-alone application** to be running, and the **aidr-tagger-api EE application** to be deployed (in the future, these will be a single EE application).

## Command line

The tagger tester is run through the following command:

```
mvn test -DtaggerTesterTest PARAMETERS
```

These parameters are optional:

```
-Dconfig=FILE
-Dnitems-train=NUMBER (default 200)
-Dnitems-test=NUMBER (default 1000)
-Dquiet=TRUE/FALSE (default false)
```

The _config_ is the name of the tagger configuration (or of a centralized configuration) to read properties that the tester needs to know to perform the testing.

The _nitems-train_ is the number of training (labelled) items to give to the tagger. It must be strictly larger than `sampleCountThreshold` which is the minimum number of items required to create a model.

The _nitems-test_ is the number of testing (unlabelled) items to give to the tagger. It can be any number greater than 1000. The reason why this cannot be a small number is to be able to have good statistics about the number of cases classified correctly and incorrectly.

The _quiet_ option suppresses the print of the tweets. All other messages are printed even in _quiet_ mode.

## Text of training/testing tweets

All tweets generated by this tester are synthetic and randomly generated, but conform to a specific format.

Half of the training tweets have the "WHITE" attribute value as a human-provided tag, and are made of random 10-word sequences of the words "light", "clear", "snow", "clouds", and "neutral".

Half of the training tweets have the "BLACK" attribute value as a human-provided tag, and are made of random 10-word sequences of the words "coal", "night", "coffee", "ink", and "neutral".

Example training tweets:

```
light clear neutral snow snow neutral clear light clear light -> WHITE
clear clouds snow light neutral snow clear clear light light -> WHITE
neutral neutral coffee coffee night coal night ink ink night -> BLACK
night coffee night coal coal ink night coffee neutral night -> BLACK
```

The testing tweets are generated in the same way, half of them correspond to "WHITE" tweets, half of them to "BLACK" tweets. Note that the testing items have no label associated to them, i.e. they are unlabelled.

The purpose of the "neutral" word is to have overlap, i.e. a word that appears in both the WHITE and BLACK sets, which avoids generating a trivial classification problem.

## Execution

The tagger tester should perform the following steps:

1. Make sure there is no data with `code="tagger_tester"` in case the tagger tester died abnormally in a previous run.
1. Create a crisis (`name="Test Crisis", code="tagger_tester"`) using the `addCrisis` service in the `CrisisResource` of the Tagger-API module.
1. Create a classifier using the following steps:
 1. Create an attribute (name="Black-White-Classifier") using the `NominalAttributeResource` in the Tagger-API module.
 1. Create three labels using the `NominalLabelResource` in the Tagger-API module (use `attribute_id` generated during the previous step)
   1. `name="White", code="white"`
    1. `name="Black", code="black"`
    1. `name="Does not apply" code="null"`
1. Create a `ModelFamily' using the `addCrisisAttribute` service of the `ModelFamilyResource` in the Tagger-API module (use crisis_id, nominal_attribute_id and nominal_label_id generated in the previous steps).
1. Subscribe to the REDIS queue where the tagger writes its output, otherwise FAIL.
1. Generate training/test items and push them to Redis on channel `FetcherChannel.test_crisis`. Crisis code is the code of your crisis created in the first step. 
1. Generate WHITE testing items and push them to the tagger **HOW?**
1. Verify that at least 80% of them are tagged WHITE, otherwise FAIL
1. Generate BLACK testing items and push them to the tagger **HOW?**
1. Verify that at least 80% of them are tagged BLACK, otherwise FAIL
1. Remove all data having `code="test_crisis"`
1. If this point is reached, exit with a successful return code

FAIL means printing a clear and informative message describing the condition and exiting with code 1 (non success).

On interrupt by the user, the classifier tester should attempt to cleanup any state created in the classifier (**HOW?**).